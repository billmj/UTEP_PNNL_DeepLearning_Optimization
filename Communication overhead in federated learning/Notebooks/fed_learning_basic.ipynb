{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aaadca0b-59cb-41c8-83c0-d14d62f0596b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type of X after transformation: <class 'numpy.ndarray'>\n",
      "Shape of X after transformation: (82332, 191)\n",
      "Final X_train shape: (65865, 191)\n",
      "Final X_test shape: (16467, 191)\n",
      "Final y_train shape: (65865,)\n",
      "Final y_test shape: (16467,)\n"
     ]
    }
   ],
   "source": [
    "# Imports for preprocessing\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "# Load and preprocess data\n",
    "data = pd.read_csv('/global/homes/b/billmj/fl_unsw/data/centralized_test_data.csv')\n",
    "\n",
    "# Separate features and target\n",
    "X = data.drop(columns=['label'])\n",
    "y = data['label']\n",
    "\n",
    "# Identify categorical columns\n",
    "categorical_columns = ['proto', 'service', 'state']\n",
    "\n",
    "# Apply one-hot encoding to categorical columns and scaling to numerical columns\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), X.select_dtypes(include=['int64', 'float64']).columns),\n",
    "        ('cat', OneHotEncoder(), categorical_columns)\n",
    "    ])\n",
    "\n",
    "X = preprocessor.fit_transform(X)\n",
    "\n",
    "# Convert sparse matrix to dense format\n",
    "X = X.toarray()\n",
    "\n",
    "# Print the type and shape of the transformed data\n",
    "print(\"Type of X after transformation:\", type(X))\n",
    "print(\"Shape of X after transformation:\", X.shape)\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Convert to numpy arrays (should be already numpy arrays, but this is to ensure)\n",
    "X_train = np.array(X_train)\n",
    "X_test = np.array(X_test)\n",
    "y_train = np.array(y_train)\n",
    "y_test = np.array(y_test)\n",
    "\n",
    "# Print shapes to verify\n",
    "print(\"Final X_train shape:\", X_train.shape)\n",
    "print(\"Final X_test shape:\", X_test.shape)\n",
    "print(\"Final y_train shape:\", y_train.shape)\n",
    "print(\"Final y_test shape:\", y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c696bed1-ad3e-4e30-a1de-5945f1a36f89",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/global/homes/b/billmj/.conda/envs/fed_unsw/lib/python3.9/site-packages/torch/cuda/__init__.py:619: UserWarning: Can't initialize NVML\n",
      "  warnings.warn(\"Can't initialize NVML\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Round 1, Test accuracy: 0.8392\n",
      "Round 2, Test accuracy: 0.9078\n",
      "Round 3, Test accuracy: 0.9374\n",
      "Round 4, Test accuracy: 0.9472\n",
      "Checkpoint saved at round 5\n",
      "Round 5, Test accuracy: 0.9531\n",
      "Round 6, Test accuracy: 0.9573\n",
      "Round 7, Test accuracy: 0.9614\n",
      "Round 8, Test accuracy: 0.9633\n",
      "Round 9, Test accuracy: 0.9647\n",
      "Checkpoint saved at round 10\n",
      "Round 10, Test accuracy: 0.9665\n",
      "Round 11, Test accuracy: 0.9689\n",
      "Round 12, Test accuracy: 0.9707\n",
      "Round 13, Test accuracy: 0.9722\n",
      "Round 14, Test accuracy: 0.9740\n",
      "Checkpoint saved at round 15\n",
      "Round 15, Test accuracy: 0.9767\n",
      "Communication metrics saved to communication_metrics.csv\n",
      "Final Test accuracy: 0.9767\n",
      "F1 Score: 0.9791\n",
      "Precision: 0.9672\n",
      "Recall: 0.9914\n",
      "ROC AUC Score: 0.9751\n",
      "Matthews Correlation Coefficient: 0.9532\n",
      "\n",
      "Confusion Matrix:\n",
      "[[7095  305]\n",
      " [  78 8989]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.96      0.97      7400\n",
      "           1       0.97      0.99      0.98      9067\n",
      "\n",
      "    accuracy                           0.98     16467\n",
      "   macro avg       0.98      0.98      0.98     16467\n",
      "weighted avg       0.98      0.98      0.98     16467\n",
      "\n",
      "\n",
      "Total execution time: 68.47 seconds\n"
     ]
    }
   ],
   "source": [
    "# Imports for federated learning\n",
    "import os\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, roc_auc_score, confusion_matrix, classification_report, matthews_corrcoef\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "# Shuffle the training data\n",
    "X_train, y_train = shuffle(X_train, y_train, random_state=42)\n",
    "\n",
    "# Split data into clients (simulating different clients)\n",
    "num_clients = 10\n",
    "client_data = []\n",
    "for i in range(num_clients):\n",
    "    start = i * len(X_train) // num_clients\n",
    "    end = (i + 1) * len(X_train) // num_clients\n",
    "    client_data.append((X_train[start:end], y_train[start:end]))\n",
    "\n",
    "# Define the model\n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 16)\n",
    "        self.dropout1 = nn.Dropout(0.3)\n",
    "        self.fc2 = nn.Linear(16, 8)\n",
    "        self.dropout2 = nn.Dropout(0.3)\n",
    "        self.fc3 = nn.Linear(8, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.dropout1(x)\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.dropout2(x)\n",
    "        x = torch.sigmoid(self.fc3(x))\n",
    "        return x\n",
    "\n",
    "# Define client update function with learning rate scheduler\n",
    "def client_update(model, train_loader, criterion, optimizer, scheduler, num_epochs):\n",
    "    model.train()\n",
    "    computation_time = 0\n",
    "    for epoch in range(num_epochs):\n",
    "        for data, target in train_loader:\n",
    "            comp_start = time.time()\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target.unsqueeze(1).float())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            comp_end = time.time()\n",
    "            computation_time += comp_end - comp_start\n",
    "        scheduler.step()\n",
    "    return model.state_dict(), computation_time\n",
    "\n",
    "# Calculate model size\n",
    "def calculate_model_size(model):\n",
    "    return sum(p.numel() * p.element_size() for p in model.parameters()) / (1024 * 1024)  # Size in MB\n",
    "\n",
    "# Implement federated learning with FedAvg and adaptive learning rate\n",
    "def federated_learning_with_fedavg(num_rounds, num_epochs_per_round, batch_size, initial_lr, checkpoint_interval=5):\n",
    "    input_dim = X_train.shape[1]\n",
    "    global_model = SimpleNN(input_dim)\n",
    "    global_weights = global_model.state_dict()\n",
    "\n",
    "    # Create checkpoint directory\n",
    "    checkpoint_dir = 'checkpoints'\n",
    "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "\n",
    "    # Initialize the communication metrics\n",
    "    communication_metrics = {\n",
    "        \"Round\": [],\n",
    "        \"Average_RTT(s)\": [],\n",
    "        \"Uplink_Data(MB)\": [],\n",
    "        \"Downlink_Data(MB)\": [],\n",
    "        \"Total_Data_Transferred(MB)\": [],\n",
    "        \"Computation_Time(s)\": [],\n",
    "        \"Communication_Time(s)\": [],\n",
    "        \"Round_Total_Time(s)\": [],\n",
    "        \"Round_Accuracy\": []\n",
    "    }\n",
    "\n",
    "    for round in range(num_rounds):\n",
    "        client_updates = []\n",
    "        total_comm_time = 0\n",
    "        total_comp_time = 0\n",
    "        total_uplink_data = 0\n",
    "        total_downlink_data = 0\n",
    "        round_start_time = time.time()\n",
    "\n",
    "        for client_x, client_y in client_data:\n",
    "            client_tensor_x = torch.tensor(client_x, dtype=torch.float32)\n",
    "            client_tensor_y = torch.tensor(client_y, dtype=torch.int64)\n",
    "            client_dataset = TensorDataset(client_tensor_x, client_tensor_y)\n",
    "            client_loader = DataLoader(client_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "            client_model = SimpleNN(input_dim)\n",
    "            \n",
    "            # Downlink: Global model to client\n",
    "            downlink_start = time.time()\n",
    "            client_model.load_state_dict(global_weights)\n",
    "            downlink_end = time.time()\n",
    "            downlink_time = downlink_end - downlink_start\n",
    "            downlink_data = calculate_model_size(client_model)\n",
    "\n",
    "            criterion = nn.BCELoss()\n",
    "            optimizer = optim.Adam(client_model.parameters(), lr=initial_lr)\n",
    "            scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.9)  # Reduce LR by 10% each epoch\n",
    "\n",
    "            # Client computation\n",
    "            new_weights, client_comp_time = client_update(client_model, client_loader, criterion, optimizer, scheduler, num_epochs_per_round)\n",
    "\n",
    "            # Uplink: Client model to server\n",
    "            uplink_start = time.time()\n",
    "            client_updates.append(new_weights)\n",
    "            uplink_end = time.time()\n",
    "            uplink_time = uplink_end - uplink_start\n",
    "            uplink_data = calculate_model_size(client_model)\n",
    "\n",
    "            total_comm_time += downlink_time + uplink_time\n",
    "            total_comp_time += client_comp_time\n",
    "            total_uplink_data += uplink_data\n",
    "            total_downlink_data += downlink_data\n",
    "\n",
    "        # Average updates from all clients (FedAvg)\n",
    "        global_weights = {k: torch.stack([client[k] for client in client_updates], 0).mean(0) for k in global_weights.keys()}\n",
    "        global_model.load_state_dict(global_weights)\n",
    "\n",
    "        # Save checkpoint\n",
    "        if (round + 1) % checkpoint_interval == 0:\n",
    "            checkpoint_path = os.path.join(checkpoint_dir, f'round_{round+1}.pth')\n",
    "            torch.save(global_model.state_dict(), checkpoint_path)\n",
    "            print(f\"Checkpoint saved at round {round + 1}\")\n",
    "\n",
    "        # Evaluate global model\n",
    "        global_model.eval()\n",
    "        test_tensor_x = torch.tensor(X_test, dtype=torch.float32)\n",
    "        test_tensor_y = torch.tensor(y_test, dtype=torch.int64)\n",
    "        with torch.no_grad():\n",
    "            test_output = global_model(test_tensor_x).squeeze()\n",
    "            test_accuracy = ((test_output > 0.5).int() == test_tensor_y).float().mean()\n",
    "\n",
    "        round_end_time = time.time()\n",
    "        round_total_time = round_end_time - round_start_time\n",
    "        avg_rtt = total_comm_time / num_clients\n",
    "\n",
    "        print(f\"Round {round + 1}, Test accuracy: {test_accuracy:.4f}\")\n",
    "\n",
    "        # Log communication metrics\n",
    "        communication_metrics[\"Round\"].append(round + 1)\n",
    "        communication_metrics[\"Average_RTT(s)\"].append(avg_rtt)\n",
    "        communication_metrics[\"Uplink_Data(MB)\"].append(total_uplink_data)\n",
    "        communication_metrics[\"Downlink_Data(MB)\"].append(total_downlink_data)\n",
    "        communication_metrics[\"Total_Data_Transferred(MB)\"].append(total_uplink_data + total_downlink_data)\n",
    "        communication_metrics[\"Computation_Time(s)\"].append(total_comp_time)\n",
    "        communication_metrics[\"Communication_Time(s)\"].append(total_comm_time)\n",
    "        communication_metrics[\"Round_Total_Time(s)\"].append(round_total_time)\n",
    "        communication_metrics[\"Round_Accuracy\"].append(test_accuracy.item())\n",
    "\n",
    "    # Save communication metrics to CSV\n",
    "    metrics_df = pd.DataFrame(communication_metrics)\n",
    "    metrics_df.to_csv('communication_metrics.csv', index=False)\n",
    "    print(\"Communication metrics saved to communication_metrics.csv\")\n",
    "\n",
    "    return global_model\n",
    "\n",
    "# Run federated learning with FedAvg and adaptive learning rate\n",
    "start_time = time.time()\n",
    "final_model = federated_learning_with_fedavg(\n",
    "    num_rounds=15,\n",
    "    num_epochs_per_round=5,\n",
    "    batch_size=256,\n",
    "    initial_lr=0.001,\n",
    "    checkpoint_interval=5\n",
    ")\n",
    "\n",
    "# Final evaluation\n",
    "final_model.eval()\n",
    "test_tensor_x = torch.tensor(X_test, dtype=torch.float32)\n",
    "test_tensor_y = torch.tensor(y_test, dtype=torch.int64)\n",
    "with torch.no_grad():\n",
    "    test_output = final_model(test_tensor_x).squeeze()\n",
    "    y_pred = (test_output > 0.5).int()\n",
    "\n",
    "# Calculate and print various metrics\n",
    "accuracy = accuracy_score(test_tensor_y, y_pred)\n",
    "f1 = f1_score(test_tensor_y, y_pred)\n",
    "precision = precision_score(test_tensor_y, y_pred)\n",
    "recall = recall_score(test_tensor_y, y_pred)\n",
    "roc_auc = roc_auc_score(test_tensor_y, y_pred)\n",
    "mcc = matthews_corrcoef(test_tensor_y, y_pred)\n",
    "\n",
    "print(f\"Final Test accuracy: {accuracy:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"ROC AUC Score: {roc_auc:.4f}\")\n",
    "print(f\"Matthews Correlation Coefficient: {mcc:.4f}\")\n",
    "\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(confusion_matrix(test_tensor_y, y_pred))\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(test_tensor_y, y_pred))\n",
    "\n",
    "total_time = time.time() - start_time\n",
    "print(f\"\\nTotal execution time: {total_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f14bb700-754e-4705-813c-911024844e28",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c9bd42f-250d-4b83-bdb4-ecab9c191c38",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d3e0419-6a6d-4d95-8fb0-8bc2ba8e1871",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a4733c1-0bcb-4bcb-a706-4e01d92b018b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cb4f223-cb34-4954-b0fc-a26f6c283160",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9812e29-0368-472d-8b3f-2db06b6cabf9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39e4ad66-3881-4313-aa55-bb8dd851d736",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0149eac-194a-4db2-aa35-45beeb220bb0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39a598cc-eec9-4f60-a33e-2917483ca665",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1174286d-b129-4aaa-b05a-b18db7e482a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fba7a36-8177-4d35-9db3-7d99608c5801",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bfe8a88-21a6-4284-bb04-34d3b99662c3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (fed_unsw)",
   "language": "python",
   "name": "fed_unsw"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
