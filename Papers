
1. Large scale performance analysis of distributed deep learning frameworks for convolutional neural network, by Aach et al., 2023
   Link: https://journalofbigdata.springeropen.com/articles/10.1186/s40537-023-00765-w#Abs1

2. Bridging the Gap Between Memory and Communication Efficiency on Distributed Deep Learning Systems, by Zhao et al., 2021
  Link: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9398682

3. Maciej Besta, Pawel Renc, Robert Gerstenberger, Paolo Sylos Labini, Alexandros Ziogas, Tiancheng Chen, Lukas Gianinazzi, 
Florian Scheidl, Kalman Szenes, Armon Carigiet, Patrick Iff, Grzegorz Kwasniewski, Raghavendra Kanakagiri, Chio Ge, Sammy Jaeger, 
Jarosław Wąs, Flavio Vella, and Torsten Hoefler. 2023. High-Performance and Programmable Attentional Graph Neural Networks 
with Global Tensor Formulations. In Proceedings of the International Conference for High Performance Computing, Networking, Storage 
and Analysis (SC '23). Association for Computing Machinery, New York, NY, USA, Article 66, 1–16. 
https://doi.org/10.1145/3581784.3607067

M. Eydenberg, M. Plagge and S. Rajamanickam, "A Comparison of Spectral and Spatial Graph Convolutional Neural Network Kernels Using 
GraphSAGE-Sparse," 2023 IEEE International Parallel and Distributed Processing Symposium Workshops (IPDPSW), 
St. Petersburg, FL, USA, 2023, pp. 189-198, doi: 10.1109/IPDPSW59300.2023.00041.
