
1. Large scale performance analysis of distributed deep learning frameworks for convolutional neural network, by Aach et al., 2023
   Link: https://journalofbigdata.springeropen.com/articles/10.1186/s40537-023-00765-w#Abs1

2. Bridging the Gap Between Memory and Communication Efficiency on Distributed Deep Learning Systems, by Zhao et al., 2021
  Link: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9398682

3. Maciej Besta, Pawel Renc, Robert Gerstenberger, Paolo Sylos Labini, Alexandros Ziogas, Tiancheng Chen, Lukas Gianinazzi, 
Florian Scheidl, Kalman Szenes, Armon Carigiet, Patrick Iff, Grzegorz Kwasniewski, Raghavendra Kanakagiri, Chio Ge, Sammy Jaeger, 
Jarosław Wąs, Flavio Vella, and Torsten Hoefler. 2023. High-Performance and Programmable Attentional Graph Neural Networks 
with Global Tensor Formulations. In Proceedings of the International Conference for High Performance Computing, Networking, Storage 
and Analysis (SC '23). Association for Computing Machinery, New York, NY, USA, Article 66, 1–16. 
https://doi.org/10.1145/3581784.3607067

4. M. Eydenberg, M. Plagge and S. Rajamanickam, "A Comparison of Spectral and Spatial Graph Convolutional Neural Network Kernels Using 
GraphSAGE-Sparse," 2023 IEEE International Parallel and Distributed Processing Symposium Workshops (IPDPSW), 
St. Petersburg, FL, USA, 2023, pp. 189-198, doi: 10.1109/IPDPSW59300.2023.00041.

5. Samuel Williams, Andrew Waterman, and David Patterson. 2009. Roofline: an insightful visual performance model for multicore 
architectures. Commun. ACM 52, 4 (April 2009), 65–76. https://doi.org/10.1145/1498765.1498785

6.  K. Z. Ibrahim et al., "Architectural Requirements for Deep Learning Workloads in HPC Environments," 2021 International Workshop 
on Performance Modeling, Benchmarking and Simulation of High Performance Computer Systems (PMBS), St. Louis, MO, USA, 2021, pp. 7-17, 
doi: 10.1109/PMBS54543.2021.00007. (Includes roofline models of Cosmoflow and DeepCAM)

7. 	Charlene Yang, Yunsong Wang, Thorsten Kurth, Steven Farrell, Samuel Williams:
Hierarchical Roofline Performance Analysis for Deep Learning Applications. SAI (2) 2021: 473-491
https://link.springer.com/chapter/10.1007/978-3-030-80126-7_35 (includes roofline models of DeepCAM)

7a. Charlene Yang, Thorsten Kurth, and Samuel Williams. Hierarchical Roofline Analysis for GPUs: Accelerating Performance
Optimization for the NERSC-9 Perlmutter System. 2019 Cray User Group meeting (CUG19). 
https://crd.lbl.gov/assets/Uploads/cug19-roofline-final.pdf

8. Mingzhen Li, Hailong Yang, Shanjun Zhang, Fengwei Yu, Ruihao Gong, Yi Liu, Zhongzhi Luan, and Depei Qian. 2023. Exploiting Subgraph 
Similarities for Efficient Auto-tuning of Tensor Programs. In Proceedings of the 52nd International Conference on Parallel Processing 
(ICPP '23). Association for Computing Machinery, New York, NY, USA, 786–796. https://doi.org/10.1145/3605573.3605596
(review presentation given by Briana in 7 June 2024 project meeting)

9. Gocht, A., Schöne, R., Frenzel, J. (2021). Advanced Python Performance Monitoring with Score-P. In: Mix, H., Niethammer, C., Zhou, 
H., Nagel, W.E., Resch, M.M. (eds) Tools for High Performance Computing 2018 / 2019. 
Springer, Cham. https://doi.org/10.1007/978-3-030-66057-4_14

10. L. WANG, W. WANG and B. LI, "CMFL: Mitigating Communication Overhead for Federated Learning," 2019 IEEE 39th International Conference 
on Distributed Computing Systems (ICDCS), Dallas, TX, USA, 2019, pp. 954-964, doi: 10.1109/ICDCS.2019.00099. 
(review presentation given by William in 14 June 2024 project meeting)

11. Ana Luisa Veroneze Solórzano and Lucas Mello Schnorr. 2022. Understanding Distributed Deep Learning Performance by Correlating HPC 
and Machine Learning Measurements. In High Performance Computing: 37th International Conference, ISC High Performance 2022, Hamburg, 
Germany, May 29 – June 2, 2022, Proceedings. Springer-Verlag, Berlin, Heidelberg, 275–292. https://doi.org/10.1007/978-3-031-07312-0_14
Companion material: https://zenodo.org/records/6349605

12. Philippe Tillet, H. T. Kung, and David Cox. 2019. Triton: an intermediate language and compiler for tiled neural network computations. 
In Proceedings of the 3rd ACM SIGPLAN International Workshop on Machine Learning and Programming Languages (MAPL 2019). 
Association for Computing Machinery, New York, NY, USA, 10–19. https://doi.org/10.1145/3315508.3329973

13. Siyu Wu, Hailong Yang, Xin You, Ruihao Gong, Yi Liu, Zhongzhi Luan, and Depei Qian. 2024. PRoof: A Comprehensive Hierarchical Profiling 
Framework for Deep Neural Networks with Roofline Analysis. In Proceedings of the 53rd International Conference on Parallel Processing 
(ICPP '24). Association for Computing Machinery, New York, NY, USA, 822–832. https://doi.org/10.1145/3673038.3673116

14. 	Hanpeng Hu, Chenyu Jiang, Yuchen Zhong, Yanghua Peng, Chuan Wu, Yibo Zhu, Haibin Lin, Chuanxiong Guo:
dPRO: A Generic Performance Diagnosis and Optimization Toolkit for Expediting Distributed DNN Training. MLSys 2022

15. Wenxue Li, Xiangzhou Liu, Yuxuan Li, Yilun Jin, Han Tian, Zhizhen Zhong, Guyue Liu, Ying Zhang, and Kai Chen. 2024. Understanding 
Communication Characteristics of Distributed Training. In Proceedings of the 8th Asia-Pacific Workshop on Networking (APNet '24). 
Association for Computing Machinery, New York, NY, USA, 1–8. https://doi.org/10.1145/3663408.3663409

16. Thiyagalingam, J., Shankar, M., Fox, G. et al. Scientific machine learning benchmarks. Nat Rev Phys 4, 413–420 (2022). 
https://doi.org/10.1038/s42254-022-00441-7




